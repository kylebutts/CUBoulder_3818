---
title: "ECON 3818"
subtitle: "Chapter 17"
author: "Kyle Butts"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default']
    self_contained: false
nature:
  highlightStyle: github
highlightLines: true
countIncrementalSlides: false
---
class: clear, middle

```{r preamble, child=here::here("Lecture Slides", "preamble.Rmd")}
```

## Chapter 17: Tests for Significance

---
# Outline

Things to cover in Chapter 17: 

- Reason we test for significance

- Stating a hypothesis

- p-value and statistical significance

- Tests for a population mean



---
# Statistical Inference

Two most common types of statistical inference:

.hi[Confidence interval]
      
- when the goal is to estimate a population parameter
      
.hi[Tests of significance]
      
- asses the evidence provided by data about a claim concerning the population
      




---
# Tests of Significance

.hi.coral[Statistical Test]: Formal procedure for comparing observed data with a claim (hypothesis) whose truth we want to assess

Basic idea: an outcome that would rarely happen if a certain claim is true, is good evidence that that claim is not true 

- .ex[Example:] say someone claims that the average points scored by a team in the NBA over the season is $50$ points. If the 2020 season has a sample mean of $80$ points scored, that's pretty good evidence against the claim.


---
# Example

Suppose that the return on Spotify stock is normally distributed with an unknown mean and a variance $\sigma^2=1$. 

You measure gains and losses for 10 trading days and collect the following sample:

$$1.6, 0.4, -2.5, 1.5, -1.1, 1.3, -0.1, -0.3, 1.2$$

<br/> 

The average rate of return is $\bar{X}=0.3$, but there are quite a few days with losses.


---
# Example

In hypothesis testing, we proceed under the assumption our claim is true and then try to disprove it.

For example we could claim ${\color{alice} \mu=0}$, that is the average Spotify return is 0%.

Since the data are drawn from a normal distribution, the sample mean is distributed $\bar{X} \sim N({\color{alice} 0}, \frac{1}{10})$.

---
# Example

In hypothesis testing, we proceed under the assumption our claim is true and then try to disprove it.

For example we could claim ${\color{alice} \mu=0}$, that is the average Spotify return is 0%.

Since the data are drawn from a normal distribution, the sample mean is distributed $\bar{X} \sim N({\color{alice} 0}, \frac{1}{10})$.

<br/>

<center>
<div class="hi">Important:</div> Note that we take the hypothesis that \({\color{alice} \mu = 0}\) as true!!
</center>

<br/>

Now we can use the sampling distribution to comment on how unlikely this claim is. 



---
# Example

```{r, echo = F, out.width = "90%"}

mean <- 0
sd <- 1/sqrt(10)

ggplot(data.frame(x = c(mean - 4*sd, mean + 4*sd)), aes(.data$x)) +
	stat_function(fun = function(x) dnorm(x, mean, sd), color = "black", size = 1.5) + 
    geom_segment(
        x = c(0.3, 1.02), y = c(0,0), xend = c(0.3, 1.02), yend = c(dnorm(0.3, mean, sd), dnorm(1.02, mean, sd)),
        color = alice, linetype = "dashed", size = 1.2
    ) +
	geom_point(
        x = c(0.3, 1.02), y = c(dnorm(0.3, mean, sd), dnorm(1.02, mean, sd)),
        color = alice, size = 2
    ) +
	kfbmisc::theme_kyle(base_size = 20) +
	labs(x = "Value", y = "Density")

```


---
# Stating a Hypothesis

It is very important to state the hypothesis clearly. A hypothesis has two parts:

.hi.alice[Null hypothesis], denoted ${\color{alice} H_0}$, is the claim we are trying to disprove
      
- test is designed to test the strength of evidence against the null hypothesis 
      
.hi.ruby[Alternative hypothesis], denoted ${\color{ruby} H_1}$, is the claim we are trying to find evidence for.sup[*]
      
- antithesis of the null

.footnote[.sup[*] Note that we are .hi[NOT] trying to prove the alternative hyptothesis is correct]
            


---
# Alternative Hypothesis

There are two types of alternative hypotheses:

One-sided alternative
      
- states the true parameter is larger *or* smaller (not both)
      
      
Two-sided alternative
      
- states the true parameter is *different* from the null


 

---
# Example

Back to the Spotify example:

Lets say we're claiming there is no return on average. We would like to see if we can disprove this hypothesis in favor of a claim that there is a positive return
$${\color{alice} H_0}: \mu=0$$
$${\color{ruby} H_1}: \mu > 0$$

.ex[Clicker Question:] What type of hypothesis test is this?
<ol type = "a">
	<li>alternative</li>
	<li>null</li>
	<li>one-sided</li>
	<li>two-sided</li>
</ol>



---
# Continuing Example

So in the Spotify example, we had 10 observations with a sample mean, $\bar{X}=0.3$. We also somehow know that $\sigma^2 = 1$. 

If we want to test the hypothesis 
$${\color{alice} H_0}: \mu=0$$
$${\color{ruby} H_1}: \mu > 0$$

<br> 

We want to calculate the probability we got an $\bar{X}$ as extreme as 0.3 if the true population $\mu=0$. Is it rare that we got 0.3 or is that quite common from the sampling distribution?

This means we want to calculate $P(\bar{X}>0.3)$ if $\bar{X} \sim N(0,\frac{1}{10})$

---
class: clear, middle

```{r, echo = F}
mean <- 0
sd <- 1/sqrt(10)

ggplot(data.frame(x = c(mean - 4*sd, mean + 4*sd)), aes(.data$x)) +
	stat_function(fun = function(x) dnorm(x, mean, sd), color = "black", size = 1.5) + 
    geom_segment(
        x = 0.3, y = 0, xend = 0.3, yend = dnorm(0.3, mean, sd),
        color = alice, linetype = "dashed", size = 1.2
    ) +
	geom_point(
        x = 0.3, y = dnorm(0.3, mean, sd),
        color = alice, size = 2
    ) +
	stat_function(
        fun = function(x) dnorm(x, mean, sd),
        xlim = c(0.3, mean + 4*sd), size = 0,
        geom = "area", fill = alice, alpha = 0.5
    ) +
	annotate(
        geom = "text", x = 0.35, y = dnorm(0.3, mean, sd),
        label = "P( bar(X) > 0.3)", parse = TRUE,
        size = 6, hjust = 0, color = "black"
    ) +
	kfbmisc::theme_kyle(base_size = 20) +
	labs(x = "Value", y = "Density")
```



---
# Continuing Example

We want to calculate $P(\bar{X}>0.3)$ if $\bar{X} \sim N(0,\frac{1}{10})$



--

$$P(\bar{X}>0.3) = P(\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} > \frac{0.3- {\color{alice} 0}}{1/\sqrt{10}}) = P(Z>0.95) = 0.1711$$

This says that if the *true* population mean is ${\color{alice} \mu = 0}$, then 17% of the time we would expect to see a sample mean greater than 0.3, 17% of the time.

  - This is not so rare (~ 1 out of 5), so this isn't strong evidence against the null. It doesn't prove it, but it does not rule it out.

---
# p-values

Suppose you calculate a test statistic $\hat{\theta}$ (e.g. mean or standard deviation) from sample data, and formulate a null hypothesis ${\color{alice} H_0}: \theta=\theta_0$ about the true value of $\theta$

- .hi.kelly[p-value] is the probability that the test statistic takes a value as or *more extreme* than the observed value $\hat{\theta}$, <span class="hi">under the assumption that</span> ${\color{alice} H_0}$ <span class="hi">is true</span><span class="sup">*</span>

<div class="footnote">
<span class="sup">*</span> Can not emphasize this enough. Under the assumption that \({\color{alice} H_0}\) is true.
</div>

--

In general, the test statistic we calculate is $\bar{X}$ and formulate a hypothesis about $\mu$

- In this context, the p-value is the probability of getting an $\bar{X}$ as or *more extreme*, if the true population mean is equal to $\mu$





---
# Calculating p-values

The way a p-value is calculated changes depending on the type of alternative hypothesis we have:

$${\color{alice} H_0: \mu = \mu_0}$$

**Left-tailed Test:** 
$${\color{ruby} H_1}: \mu> \mu_0 \ \implies p = P(\bar{X} \leq \bar{X}_{obs} \ \big\vert \  {\color{alice} \mu = \mu_0})$$

--
**Right-tailed Test:**

$${\color{ruby} H_1}: \mu< \mu_0 \ \implies  p = P(\bar{X} \geq \bar{X}_{obs} \ \big\vert \ {\color{alice} \mu = \mu_0})$$

--
**Two-tailed Test:**

$${\color{ruby} H_1}: \mu \neq \mu_0 \  \implies p = 2 \cdot P(\bar{X} \geq \vert\bar{X}_{obs} \vert \ \big\vert \ {\color{alice} \mu = \mu_0})$$

where $\bar{X}$ is a random draw from our sample distribution and $\bar{X}_{obs}$ is our **observed** value for the sample statistic.




---
# Example -- One-sided

Lets say we have a sample of 30 observations and calculate a sample mean of 1020. We also know that the standard deviation is $\sigma=40$. We want to test the following hypothesis:
$${\color{alice} H_0}: \mu=1000$$
$${\color{ruby} H_1}: \mu > 1000$$

- Write out p-value:

--

$$p = P(\bar{X} \geq 1020 \ \big| \ \mu=1000)$$

- Solve for p-value:

--
$$P(\frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}}\geq \frac{1020-1000}{40/\sqrt{30}} )= P(Z>2.74)=0.0032$$

So $p=0.003$, the probability we get a sample mean as large as 1020 if the true mean is 1000 is .3%, super rare!



---
# Example -- Two-sided

Say we have nine observations, and calculate a sample mean of $\bar{X} = 0.5$. We know the underlying distribution has a standard deviation $\sigma = 1$. We want to test the following hypothesis:

$${\color{alice} H_0}: \mu=0$$
$${\color{ruby} H_1}: \mu \neq 0$$

- Write out p-value:

--

$$p=2 \cdot P(\bar{X} \geq 0.5 \ \big| \ \mu=0)$$

- Solve for p-value:

--

$$2 \cdot P(\frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}}\geq \frac{0.5-0}{1/\sqrt{9}} )=2 \cdot P(Z>1.5)=2\cdot(0.067)=0.134$$

So $p=0.134$, meaning there is a 13.4% probability of getting a sample mean *as extreme as* 0.5 if the true mean is 0.


---
# Breakout Group
Say we have a sample of 25 observations and we calculate a sample mean of 11,500. We know that the standard deviation is $\sigma=5000$. We want to test the following hypothesis:
$${\color{alice} H_0}: \mu=12,675$$
$${\color{ruby} H_1}: \mu < 12,675$$

Calculate the p-value:
<ol type = "a">
	<li>0.407</li>
	<li>0.814</li>
	<li>0.119</li>
	<li>0.238</li>
</ol>


---
# Breakout Group

Say we have a sample of 25 observations and we calculate a sample mean of 11,500. We know that the standard deviation is $\sigma=5000$. We want to test the following hypothesis:

$${\color{alice} H_0}: \mu=12,675$$
$${\color{ruby} H_1}: \mu < 12,675$$



---
# Hypothesis Testing

A restaurant owner is determining whether to expand her restaurant to another location. She counts the number of customers per weekend and calculates an average of 538 customers over 50 weekends. She's heard that restaurants of her size shouldn't expand unless they get more than 510 customers each weekend. Conduct a hypothesis at the $\alpha=0.05$ level and determine whether or not she can feel confident she gets at least 510 customers each weekend. Assume $\sigma= 120$.

- State Null and Alternative hypotheses

--

$${\color{alice} H_0}: \mu=510$$
$${\color{ruby} H_1}: \mu \geq 510$$
      
      

---
# Hypothesis Testing

A restaurant owner is determining whether to expand her restaurant to another location. She counts the number of customers per weekend and calculates an average of 538 customers over 50 weekends. She's heard that restaurants of her size shouldn't expand unless they get more than 510 customers each weekend. Conduct a hypothesis at the $\alpha=0.05$ level and determine whether or not she can feel confident she gets at least 510 customers each weekend. Assume $\sigma= 120$.

- Calculate p-value

--

$P(\bar{X} \geq 538 | \mu=510)=0.049$
      
If true population mean is 510, there is a 4.95% probability of calculating a sample mean as large as 538


---
# Interpreting p-Values

Small p-values are evidence against the null, because they say **the observed value of the sample statistic if very unlikely to occur if the null hypothesis,** ${\color{alice} H_0}$, **is true.**

- Larger p-values fail to give evidence against the null because the observed sample statistic could have resulted by chance if the null hypothesis, ${\color{alice} H_0}$ is true. 

--

<br/>
<center>
p-values do not tell you the probability that \({\color{alice} H_0}\) is true!!<span class="sup">*</span>
</center>

.footnote[.sup[*] I will likely ask this on the exam, so make sure to understand why]



---
# p-Values and Statistical Significance

To review, our decision process using p-values is

- p-value is sufficiently small $\rightarrow$ reject ${\color{alice} H_0} \rightarrow$ accept ${\color{ruby} H_1}$
- p-value is too large $\rightarrow$ fail to reject ${\color{alice} H_0} \rightarrow$ do not accept ${\color{ruby} H_1}$



We *never* say "accept ${\color{alice} H_0}$". All we can say is if there is enough evidence to say it is false, we cannot know if it is true.

We say our results are .hi.purple[statistically significant] if we reject ${\color{alice} H_0}$.sup[*]

<div class="footnote">
<span class="sup">*<span> This saying is short for "statistically significantly different from zero" (or \(H_0\) more generally).
</div>


---
# p-Values and Statistical Significance

How small does p need to be in order to be "sufficiently small"? When can we claim our results are statistically significant?

Suppose you calculate a p-value, and specify a .hi.purple[level of significance], $\alpha$. We say our results are .hi.purple[statistically significant] if:
$$\text{p-value} \leq {\color{purple}\alpha}$$

We will discuss how to think about choosing $\alpha$. Some of the most common levels of significance are .01, .05, and 0.1.



---
# Group Example

A restaurant owner is determining whether to expand her restaurant to another location. She counts the number of customers per weekend and calculates an average of 538 customers over 50 weekends. She's heard that restaurants of her size shouldn't expand unless they get more than 510 customers each weekend. Conduct a hypothesis at the $\alpha=0.05$ level and determine whether or not she can feel confident she gets at least 510 customers each weekend. Assume $\sigma= 120$.

1. State Null and Alternative hypotheses

2. Calculate p-value

3. Is our estimate statistically siginificant?

---
# Group Example 

1. State Null and Alternative hypotheses

	- ${\color{alice} H_0}: \mu=510; \quad {\color{ruby} H_1}: \mu \geq 510$
      
2. Calculate p-value 
	
	- $P(\bar{X} \geq | \mu=510)=0.049$
      
3. Is our estimate statistically siginificant?
      
	- $0.049 < 0.05 \implies p-value<\alpha \implies \text{Reject } {\color{alice} H_0}$
      

Reject null hypothesis in favor of alternative, that the true population mean number of customers is greater than 510


---
# Hypothesis Testing Algorithm

A hypothesis test consists of 4 parts:

- Null hypothesis, ${\color{alice} H_0}$

- Alternative hypothesis, ${\color{ruby} H_1}$

- Test statistic, $\hat{\theta}$, most commonly $\bar{X}$

- Level of significances $\alpha$


With these, we reach the conclusion:
      
- reject ${\color{alice} H_0}$ if $p \leq \alpha$

- fail to reject ${\color{alice} H_0}$ if $p > \alpha$
      





---
# Rejection Region


Imagine getting many many observed sample means $(\bar{X}_{1, obs}, \bar{X}_{2, obs}, \dots, \bar{X}_{100, obs})$. We don't want to calculate 100 $p$-values.

It would be better if we could figure out for what range of values that would give us $p$-value less than $\alpha$.


---
# Rejection Region


```{r rejection-region, echo = F, out.width = "90%"}
mean <- 0
sd <- 1/sqrt(10)

ggplot(data.frame(x = c(mean - 4*sd, mean + 4*sd)), aes(.data$x)) +
	stat_function(fun = function(x) dnorm(x, mean, sd), color = "black", size = 1.5) + 
    geom_segment(
        x = qnorm(0.95, mean, sd), y = 0, xend = qnorm(0.95, mean, sd), yend = dnorm(qnorm(0.95, mean, sd), mean, sd),
        color = alice, linetype = "dashed", size = 1.2
    ) +
	geom_point(
        x = qnorm(0.95, mean, sd), y = dnorm(qnorm(0.95, mean, sd), mean, sd),
        color = alice, size = 2
    ) +
	stat_function(
        fun = function(x) dnorm(x, mean, sd),
        xlim = c(qnorm(0.95, mean, sd), mean + 4*sd), size = 0,
        geom = "area", fill = alice, alpha = 0.5
    ) +
	annotate(
        geom = "text", x = qnorm(0.95, mean, sd) + 0.05, y = dnorm(qnorm(0.95, mean, sd), mean, sd),
        label = expression(P( bar(X) > X^"*") == 0.05), parse = TRUE,
        size = 6, hjust = 0, color = "black"
    ) +
	scale_x_continuous(
		breaks = c(-1, 0, qnorm(0.95, mean, sd), 1),
		labels = c("-1", "0", expression(X^"*"), "1")
	) +
	kfbmisc::theme_kyle(base_size = 20) +
	labs(x = "Value", y = "Density") + 
	theme(panel.grid.minor.x = element_blank())

```


---
# Rejection Region


```{r rejection-region-ex1, echo = F, out.width = "90%"}

mean <- 0
sd <- 1/sqrt(10)
xobs <- 0.25


ggplot(data.frame(x = c(mean - 4*sd, mean + 4*sd)), aes(.data$x)) +
	# Density
	stat_function(fun = function(x) dnorm(x, mean, sd), color = "black", size = 1.5) + 
	# X_obs
    geom_segment(
        x = xobs, y = 0, xend = xobs, yend = dnorm(xobs, mean, sd),
        color = ruby, linetype = "dashed", size = 1.2
    ) +
	geom_point(
        x = xobs, y = dnorm(xobs, mean, sd),
        color = ruby, size = 2
    ) +
	stat_function(
        fun = function(x) dnorm(x, mean, sd),
        xlim = c(xobs, mean + 4*sd), size = 0,
        geom = "area", fill = ruby, alpha = 0.5
    ) +
	# X^*
    geom_segment(
        x = qnorm(0.95, mean, sd), y = 0, xend = qnorm(0.95, mean, sd), yend = dnorm(qnorm(0.95, mean, sd), mean, sd),
        color = "black", linetype = "dashed", size = 1.2
    ) +
	geom_point(
        x = qnorm(0.95, mean, sd), y = dnorm(qnorm(0.95, mean, sd), mean, sd),
        color = "black", size = 2
    ) +
	annotate(
        geom = "text", x = qnorm(0.95, mean, sd) + 0.05, y = dnorm(qnorm(0.95, mean, sd), mean, sd),
        label = expression(P( bar(X) > X^"*") == 0.05), parse = TRUE,
        size = 6, hjust = 0, color = "black"
    ) +
	scale_x_continuous(
		breaks = c(-1, 0, xobs, qnorm(0.95, mean, sd), 1),
		labels = c("-1", "0", expression(X[obs]), expression(X^"*"), "1")
	) +
	kfbmisc::theme_kyle(base_size = 20) +
	labs(x = "Value", y = "Density") + 
	theme(panel.grid.minor.x = element_blank())

```



---
# Rejection Region


```{r rejection-region-ex2, echo = F, out.width = "90%"}

mean <- 0
sd <- 1/sqrt(10)
xobs <- 0.75


ggplot(data.frame(x = c(mean - 4*sd, mean + 4*sd)), aes(.data$x)) +
	# Density
	stat_function(fun = function(x) dnorm(x, mean, sd), color = "black", size = 1.5) + 
	# X_obs
    geom_segment(
        x = xobs, y = 0, xend = xobs, yend = dnorm(xobs, mean, sd),
        color = ruby, linetype = "dashed", size = 1.2
    ) +
	geom_point(
        x = xobs, y = dnorm(xobs, mean, sd),
        color = ruby, size = 2
    ) +
	stat_function(
        fun = function(x) dnorm(x, mean, sd),
        xlim = c(xobs, mean + 4*sd), size = 0,
        geom = "area", fill = ruby, alpha = 0.5
    ) +
	# X^*
    geom_segment(
        x = qnorm(0.95, mean, sd), y = 0, xend = qnorm(0.95, mean, sd), yend = dnorm(qnorm(0.95, mean, sd), mean, sd),
        color = "black", linetype = "dashed", size = 1.2
    ) +
	geom_point(
        x = qnorm(0.95, mean, sd), y = dnorm(qnorm(0.95, mean, sd), mean, sd),
        color = "black", size = 2
    ) +
	annotate(
        geom = "text", x = qnorm(0.95, mean, sd) + 0.05, y = dnorm(qnorm(0.95, mean, sd), mean, sd),
        label = expression(P( bar(X) > X^"*") == 0.05), parse = TRUE,
        size = 6, hjust = 0, color = "black"
    ) +
	scale_x_continuous(
		breaks = c(-1, 0, xobs, qnorm(0.95, mean, sd), 1),
		labels = c("-1", "0", expression(X[obs]), expression(X^"*"), "1")
	) +
	kfbmisc::theme_kyle(base_size = 20) +
	labs(x = "Value", y = "Density") + 
	theme(panel.grid.minor.x = element_blank())

```

---
# Rejection Region

We call the range of values for our *observed* test statistic $\bar{X}_{obs}$ which we would reject the null the .hi.purple[rejection region]

Calculate rejection region using

- the population mean, $\mu$ under the null hypothesis

- the standard deviation, $\sigma$

- level of significance, $\alpha$ and Z-score associated with it

The rejection region tells us how extreme the sample mean needs to be for us to reject the null  hypothesis ${\color{alice} H_0}: \mu = \mu_0$
---
# Calculate Rejection Region -- One-Sided

Let's say we want to test a hypothesis about the number of children in a family. We have a sample of 25 observations, and we know $\sigma=0.8$. We decide to test the following hypothesis:

$${\color{alice} H_0}:\mu=2$$

$${\color{ruby} H_1}:\mu<2$$

If we are testing at the 5% significance level, calculate the rejection region. 

- The rejection region is the values of $\bar{X}$ that are far enough away from $\mu$ that we would reject ${\color{alice} H_0}$

--

We want to find $\bar{X}^*$ that solves 

$$R = P(\bar{X} \leq \bar{X}^* \ \vert \ \mu_0 =2) = 0.05$$






---
# Calculating Rejection Region -- One-Sided

$$P(\bar{X}\leq\bar{X}^* \vert \mu_0 = 2) = 0.05 = P(Z\leq\frac{\bar{X}^*-\mu}{\sigma/\sqrt{n}})$$

Plugging in $\mu$ and $\sigma$, we have:

$$P(Z\leq\frac{\bar{X}^*-2}{0.8/5})=0.05=P(Z\leq\frac{\bar{X}^*-2}{.16})=0.05$$

--

We know that $P(Z \leq -1.645)=0.05$, therefore:

$$\frac{\bar{X}^*-2}{0.16}=-1.645 \implies \bar{X}^*=1.74$$

This means our rejection region is all $\bar{X}\leq1.74$. In words, we will reject ${\color{alice} H_0}:\mu=2$ at the $\alpha=0.05$ level if we calculate a sample mean of 1.74 or smaller.



---
# Breakout Group
A study is conducted to determine the average income of 18-25 year olds in San Antonio. They are trying to test whether the average is greater than \$21,000. The study is able to collect 16 different salaries. We somehow know that the population standard deviation is $\sigma=$\$2000. What sample averages would we reject the null hypothesis at the $\alpha=0.05$ level?

$${\color{alice} H_0}: \mu = \$21,000$$

$${\color{ruby} H_1}: \mu > \$21,000$$

<ol type = "a">
	<li> Reject if \( \bar{X} < \$21,822.5 \) </li>
	<li> Reject if \( \bar{X} > \$21,822.5 \) </li>
	<li> Reject if \( \bar{X} < \$20,177.5 \) </li>
	<li> Reject if \( \bar{X} >\$20,177.5 \) </li>
</ol>


---
# Breakout Group
A study is conducted to determine the average income of 18-25 year olds in San Antonio. They are trying to test whether the average is greater than \$21,000. The study is able to collect 16 different salaries. We somehow know that the population standard deviation is $\sigma=$\$2000. What sample averages would we reject the null hypothesis at the $\alpha=0.05$ level?

$${\color{alice} H_0}: \mu = \$21,000$$

$${\color{ruby} H_1}: \mu > \$21,000$$


---
# Calculating Rejection Region -- Two-Sided

Solving for a rejection region changes a little bit when we consider a two-tailed test.

Example: Suppose we have a sample of 25 observations, we know $\sigma=3.5$. We want to test the following hypothesis at the $\alpha=0.05$ level. 

$${\color{alice} H_0}: \mu=13.5$$
$${\color{ruby} H_1}: \mu \neq 13.5$$

Our rejection region is calculated by finding two different $\bar{X}^*$'s.

$$P(\bar{X}\geq\bar{X}^*_U)=\frac{0.05}{2}$$
$$P(\bar{X}\leq\bar{X}^*_L)=\frac{0.05}{2}$$






---
# Calculating Rejection Region -- Two-Sided
$$P(\bar{X}\geq\bar{X}^*_U)= \frac{0.05}{2} = 0.025 = P(Z>\frac{\bar{X}^*_U-13.5}{0.64})$$

- We know $P(Z>1.96)=0.025$. 

- Therefore: $$\frac{\bar{X}^*_U-13.5}{0.64}=1.96 \implies \bar{X}^*_U=14.75$$



---
# Breakout Group

For the same problem, solve for $\bar{X}^*_L$.

Remember it solves: $P(\bar{X}\leq\bar{X}^*_L)=\frac{0.05}{2}$

<ol type = "a">
	<li> -14.75 </li>
	<li> 12.246 </li>
	<li> 11.54 </li>
</ol>




---
# Solving Hypothesis Testing Problem

**p-value method**
      
- calculate p-value, varies based off type of alternative hypothesis
- p-value: probability of getting as extreme of a sample statistic, if the null hypothesis is true

<center>
If \(p<\alpha \implies\) reject \( {\color{alice} H_0} \)
</center>
            
      
**Rejection region method**
      
- calculate rejection region based off level of significance
- compare sample statistic with rejection region

<center>            
If sample statistic is in rejection region, reject \( {\color{alice} H_0} \)
</center>
      

